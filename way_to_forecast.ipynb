{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FckMbI10KIVo"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from statsmodels.tsa.stattools import pacf\n",
        "from statsmodels.tsa.stattools import acf\n",
        "\n",
        "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
        "from statsmodels.stats.diagnostic import kstest_normal\n",
        "from statsmodels.stats.diagnostic import het_breuschpagan\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from statsmodels.graphics.tsaplots import plot_pacf\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from math import sqrt\n",
        "from math import floor\n",
        "from math import isnan\n",
        "from scipy import stats\n",
        "from datetime import date\n",
        "from datetime import datetime\n",
        "\n",
        "from itertools import product\n",
        "from itertools import groupby\n",
        "\n",
        "!pip install arch\n",
        "from arch import arch_model\n",
        "\n",
        "from scipy import optimize\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "import warnings\n",
        "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
        "warnings.simplefilter('ignore', ConvergenceWarning)\n",
        "warnings.filterwarnings('ignore', message=\"RuntimeWarning: Values in x were outside bounds during a minimize step, clipping to bounds\")\n",
        "warnings.filterwarnings('ignore', message=\"Values in x were outside bounds during a minimize step, clipping to bounds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Data and Compute Log Returns\n",
        "\n",
        "In this section, we import the historical prices of IUIT.L from Yahoo Finance using the yfinance library. We then compute daily log returns by taking the natural logarithm of the closing prices and subtracting the previous day's log price. To remove outliers, we remove any returns that are more than 3 standard deviations from the mean. This is done to ensure that our analysis is not skewed by extreme events."
      ],
      "metadata": {
        "id": "bZZ2Naqh7MDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def readYfData(idx):\n",
        "  start_date = datetime(2016, 1, 1)\n",
        "  end_date   = date.today()\n",
        "  etf_df = yf.download(idx, start=start_date, end=end_date)\n",
        "  return etf_df\n",
        "\n",
        "def computeReturns(prices):\n",
        "  diffs   = np.diff(prices).tolist()\n",
        "  prices  = np.delete(prices, len(prices) - 1).tolist()\n",
        "  returns = [round(a / b, 7) for a,b in zip(diffs, prices)]\n",
        "  returns = [0.00000] + returns\n",
        "  return returns\n",
        "\n",
        "def removeOutliers(returns, sigma_limit):\n",
        "  scaler = StandardScaler()\n",
        "  scaled_log_returns = scaler.fit_transform(np.array(returns).reshape(-1, 1))\n",
        "  scaled_outliers = [i for i, x in enumerate(scaled_log_returns)\\\n",
        "                     if abs(x) > sigma_limit]\n",
        "  rep = np.median(returns)\n",
        "  log_returns = [rep if i in scaled_outliers else x for\\\n",
        "                 i, x in enumerate(returns)]\n",
        "  return log_returns"
      ],
      "metadata": {
        "id": "_W7aGdeWKUxW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iuit_l = readYfData('IUIT.L')\n",
        "print('Last day loaded = {}'.format(iuit_l.index[-1]))"
      ],
      "metadata": {
        "id": "SAHhMUgWKX8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define constants that help us control the training process:"
      ],
      "metadata": {
        "id": "Yn18ewHU89mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants to control trading\n",
        "ENTRY_IDX   = 1489 # Index of 19-11-2021 when we started trading\n",
        "LOOK_BACK   =  365 # Number of days to look back when building a model\n",
        "LOOK_AHEAD  =    3 # Number of days to forecast ahead\n",
        "BURN_IN     =   30 # Burn in period to allow RSI to stabilize\n",
        "OUTLIER_SIG =  3.3 # Outlier sigma to cut-off outliers\n",
        "INITIAL_INV = 1000 # Initial investment amount\n",
        "TRANS_COST  =    8 # Cost per one transaction\n",
        "IDLE_COST   =  120 # Cost if no transactions in 365 days\n",
        "MIN_PURCHAS =   10 # Minimal number of stocks to buy in a batch\n",
        "MAX_P       =    1\n",
        "MAX_Q       =    1\n",
        "\n",
        "prices      = iuit_l['Close'].tolist()\n",
        "log_prices  = np.log(prices).tolist()\n",
        "returns     = computeReturns(log_prices)\n",
        "price_trend = pd.Series(prices).rolling(100).mean()\n",
        "\n",
        "# We use standardized log returns to detect outliers since other methods like\n",
        "# IQR are too sensitive to outliers and identify too many of them in the series\n",
        "# which might lead to loosing some valuable information.\n",
        "returns  = removeOutliers(returns, OUTLIER_SIG)\n",
        "n_steps  = len(returns) - (ENTRY_IDX + LOOK_AHEAD)\n",
        "last_idx = len(returns) + 1 - LOOK_AHEAD\n",
        "\n",
        "# Note that the last index must be the length of history + 1 minus look ahead period\n",
        "# to confirm that the split was done correctly\n",
        "print('Steps = {}, history = {}, last index = {}, look ahead = {}'.format(\n",
        "    n_steps, len(returns), last_idx, LOOK_AHEAD))"
      ],
      "metadata": {
        "id": "mhyArnOcKjPH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6bf847e-d3b2-4aba-f4b0-7490b8a1400b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps = 549, history = 2041, last index = 2039, look ahead = 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Establish a Baseline Performance Benchmark\n",
        "\n",
        "To evaluate the effectiveness of our algorithmic trading strategies, a baseline performance benchmark is established using a buy-and-hold strategy. This benchmark represents the potential profit margin achievable by simply purchasing and holding IUIT.L shares over the entire historical period. The Relative Strength Index (RSI) indicator is employed to determine buy and sell signals, ensuring that our benchmark reflects a prudent approach to stock market participation."
      ],
      "metadata": {
        "id": "Gk1XmrZR-dJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def computeRsi(returns):\n",
        "  up_close   = len(list(filter(lambda x: (x > 0), returns)))\n",
        "  down_close = len(list(filter(lambda x: (x < 0), returns)))\n",
        "  if down_close == 0:\n",
        "    down_close = 1\n",
        "  rs_val = round(up_close / down_close, 2)\n",
        "  rsi    = round(100 - (100 / (1 + rs_val)), 2)\n",
        "  return rsi\n",
        "\n",
        "def calculateAverageReturn(daily):\n",
        "  cumul_r = 1\n",
        "  for r in daily:\n",
        "    cumul_r *= (1 + r)\n",
        "  return cumul_r"
      ],
      "metadata": {
        "id": "fySAgztv7b45"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Naive trading baseline -- DO NOT DELETE, as we need it for comparison\n",
        "cash   = INITIAL_INV\n",
        "stocks = 0\n",
        "\n",
        "equity    = []\n",
        "inventory = []\n",
        "\n",
        "for step in range(ENTRY_IDX, last_idx):\n",
        "  today_price = prices[step]\n",
        "  window_ret  = returns[step - LOOK_BACK:step]\n",
        "  rsi = computeRsi(window_ret[-7:])\n",
        "  signal = 'hold'\n",
        "  if rsi >= 80:\n",
        "    signal = 'sell'\n",
        "  elif rsi <= 30:\n",
        "    signal = 'buy'\n",
        "\n",
        "  if signal == 'buy':\n",
        "    affordable_qty = floor((cash - TRANS_COST) / today_price)\n",
        "    if affordable_qty >= MIN_PURCHAS:\n",
        "      cash   -= today_price * affordable_qty\n",
        "      stocks += affordable_qty\n",
        "  if signal == 'sell':\n",
        "    if stocks > 0:\n",
        "      cash  += today_price * stocks\n",
        "      stocks = 0\n",
        "  equity.append(cash)\n",
        "  inventory.append(stocks)\n",
        "\n",
        "  if step == last_idx - 1:\n",
        "    today_date = iuit_l.index[step]\n",
        "    print('Date = {}, price = {:.2f}, signal = {}'.format(today_date,\n",
        "                                                          today_price,\n",
        "                                                          signal))\n",
        "    cash  += today_price * stocks\n",
        "    stocks = 0\n",
        "\n",
        "profit = 100 * (cash - INITIAL_INV) / INITIAL_INV\n",
        "print('Profit: naive baseline = {:.2f}%'.format(profit))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYAmN3A-bfx_",
        "outputId": "5ef1897e-9ede-49b5-8378-a012bd251eb8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date = 2024-01-29 00:00:00, price = 26.13, signal = hold\n",
            "Profit: naive baseline = 14.45%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the Moving Average Convergence Divergence Strategy\n",
        "\n",
        "To enhance our trading performance beyond the buy-and-hold benchmark, we employ the Moving Average Convergence Divergence (MACD) indicator. This technical analysis tool assesses the relationship between two exponential moving averages (EMAs) to identify potential trend reversals. By analyzing the crossovers of the MACD and signal lines, we generate buy and sell signals that aim to capitalize on favorable market conditions and outperform the benchmark strategy."
      ],
      "metadata": {
        "id": "vcxHeSSEANiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MACD trading attempt to perform better than the market\n",
        "# Updates the baseline that we have to beat with machine learning\n",
        "ema12 = pd.Series(prices).ewm(12).mean()\n",
        "ema26 = pd.Series(prices).ewm(26).mean()\n",
        "macd  = ema12 - ema26\n",
        "ema9  = macd.ewm(9).mean()\n",
        "macd  = macd.tolist()\n",
        "ema9  = ema9.tolist()\n",
        "\n",
        "cash   = INITIAL_INV\n",
        "stocks = 0\n",
        "\n",
        "equity    = []\n",
        "inventory = []\n",
        "signals   = []\n",
        "\n",
        "for step in range(ENTRY_IDX, last_idx):\n",
        "  today_price = prices[step]\n",
        "  macd_yesterday = macd[step - 1] - ema9[step]\n",
        "  macd_today = macd[step] - ema9[step]\n",
        "  signal = 'hold'\n",
        "  if macd_yesterday > 0 and macd_today < 0:\n",
        "    signal = 'buy'\n",
        "  elif macd_yesterday < 0 and macd_today > 0:\n",
        "    signal = 'sell'\n",
        "  if signal == 'buy':\n",
        "    affordable_qty = floor((cash - TRANS_COST) / today_price)\n",
        "    if affordable_qty >= MIN_PURCHAS:\n",
        "      cash -= today_price * affordable_qty\n",
        "      stocks += affordable_qty\n",
        "  if signal == 'sell':\n",
        "    if stocks > 0:\n",
        "      cash += today_price * stocks\n",
        "      stocks = 0\n",
        "  equity.append(cash)\n",
        "  inventory.append(stocks)\n",
        "  signals.append(signal)\n",
        "\n",
        "profit = 100 * (cash - INITIAL_INV) / INITIAL_INV\n",
        "print('Profit: MACD crossover = {:.2f}%'.format(profit))"
      ],
      "metadata": {
        "id": "Oht4ifmjTtOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Current trading example allowing to show the progress of the trading and the last day signal together with the price"
      ],
      "metadata": {
        "id": "cqGzUQICDWbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plotPrices(prices, signals, entry_idx, start, end):\n",
        "  \"\"\"\n",
        "  This function plots price and the signals produced by the trading algorithm\n",
        "  \"\"\"\n",
        "  if start == None:\n",
        "    start = 0\n",
        "  if end == None:\n",
        "    end = len(signals)\n",
        "  subset_prices = prices[ENTRY_IDX:]\n",
        "  subset_prices = subset_prices[start:end]\n",
        "  subset_signal = signals[start:end]\n",
        "  subset_index  = range(len(subset_prices))\n",
        "\n",
        "  sub_dt = pd.DataFrame({'price':subset_prices, 'signal':subset_signal})\n",
        "  x1 = sub_dt[sub_dt['signal'] == 'buy'].index\n",
        "  y1 = sub_dt[sub_dt['signal'] == 'buy']['price']\n",
        "  x2 = sub_dt[sub_dt['signal'] == 'sell'].index\n",
        "  y2 = sub_dt[sub_dt['signal'] == 'sell']['price']\n",
        "\n",
        "  plt.plot(subset_index, subset_prices, color='lightgrey', zorder=0)\n",
        "  plt.scatter(x1, y1, marker='o', s=6, color='red', zorder=1, label='buy')\n",
        "  plt.scatter(x2, y2, marker='o', s=6, color='green', zorder=1, label='sell')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "NxXLbdJOKCsP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MACD crossover trading until the last day\n",
        "ema12 = pd.Series(prices).ewm(12).mean()\n",
        "ema26 = pd.Series(prices).ewm(26).mean()\n",
        "macd  = ema12 - ema26\n",
        "ema9  = macd.ewm(9).mean()\n",
        "macd  = macd.tolist()\n",
        "ema9  = ema9.tolist()\n",
        "\n",
        "cash   = INITIAL_INV\n",
        "stocks = 0\n",
        "\n",
        "equity    = []\n",
        "inventory = []\n",
        "signals   = []\n",
        "\n",
        "for step in range(ENTRY_IDX, len(returns)):\n",
        "  today_price = prices[step]\n",
        "  macd_yesterday = macd[step - 1] - ema9[step]\n",
        "  macd_today = macd[step] - ema9[step]\n",
        "  signal = 'hold'\n",
        "  if macd_yesterday > 0 and macd_today < 0:\n",
        "    signal = 'buy'\n",
        "  elif macd_yesterday < 0 and macd_today > 0:\n",
        "    signal = 'sell'\n",
        "  if signal == 'buy':\n",
        "    affordable_qty = floor((cash - TRANS_COST) / today_price)\n",
        "    if affordable_qty >= MIN_PURCHAS:\n",
        "      cash -= today_price * affordable_qty\n",
        "      stocks += affordable_qty\n",
        "  if signal == 'sell':\n",
        "    if stocks > 0:\n",
        "      cash += today_price * stocks\n",
        "      stocks = 0\n",
        "  equity.append(cash)\n",
        "  inventory.append(stocks)\n",
        "  signals.append(signal)\n",
        "\n",
        "today_date = iuit_l.index[step]\n",
        "print('Date = {}, price = {:.2f}, signal = {}'.format(today_date,\n",
        "                                                      today_price,\n",
        "                                                      signal))"
      ],
      "metadata": {
        "id": "wjGE9qimDaGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotPrices(prices, signals, ENTRY_IDX, None, None)"
      ],
      "metadata": {
        "id": "rO5oDIoxMHba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A trading strategy incorporating MACD and trend analysis involves immediate selling upon a trend reversal from uptrend to downtrend, provided the current price exceeds the last purchase price. In a rising market, standard MACD crossovers are utilized. This approach mitigates losses by avoiding conflicting buy and sell signals during market downturns. However, its reliance on a rolling mean trend indicator may lead to missed opportunities in situations where the price is declining despite a positive trend indication."
      ],
      "metadata": {
        "id": "sbIuKch7R8gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trend = [0 if isnan(x) else x for x in pd.Series(prices).rolling(100).mean().tolist()]\n",
        "trend_diffs = [0] + np.diff(trend).tolist()\n",
        "trend_direction = ['up' if x > 0 else 'down' if x < 0 else 'neutral' for x in trend_diffs]\n",
        "\n",
        "for x in range(1, len(trend_direction) - 1):\n",
        "  yesterday = trend_direction[x - 1]\n",
        "  today     = trend_direction[x]\n",
        "  tomorrow  = trend_direction[x + 1]\n",
        "  if yesterday == tomorrow:\n",
        "    today = yesterday\n",
        "  trend_direction[x] = today\n",
        "\n",
        "trend = trend[ENTRY_IDX:]\n",
        "trend_direction = trend_direction[ENTRY_IDX:]\n",
        "tmp = pd.DataFrame({'price':trend, 'trend':trend_direction})\n",
        "\n",
        "plt.plot(trend, color='lightgrey')\n",
        "plt.plot(np.where(tmp['trend'] == 'down', tmp['price'], None), color='red')\n",
        "plt.plot(np.where(tmp['trend'] == 'up', tmp['price'], None), color='green')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Mw0bLgyfXyTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trend = [0 if isnan(x) else x for x in pd.Series(prices).rolling(100).mean().tolist()]\n",
        "trend_diffs = [0] + np.diff(trend).tolist()\n",
        "trend_direction = ['up' if x > 0 else 'down' if x < 0 else 'neutral' for x in trend_diffs]\n",
        "\n",
        "for x in range(1, len(trend_direction) - 1):\n",
        "  yesterday = trend_direction[x - 1]\n",
        "  today     = trend_direction[x]\n",
        "  tomorrow  = trend_direction[x + 1]\n",
        "  if yesterday == tomorrow:\n",
        "    today = yesterday\n",
        "  trend_direction[x] = today\n",
        "\n",
        "ema12 = pd.Series(prices).ewm(12).mean()\n",
        "ema26 = pd.Series(prices).ewm(26).mean()\n",
        "macd  = ema12 - ema26\n",
        "ema9  = macd.ewm(9).mean()\n",
        "macd  = macd.tolist()\n",
        "ema9  = ema9.tolist()\n",
        "\n",
        "equity    = []\n",
        "inventory = []\n",
        "signals   = []\n",
        "\n",
        "cash   = INITIAL_INV\n",
        "stocks = 0\n",
        "\n",
        "last_sales_price = float('inf')\n",
        "last_buy_price = 0.0\n",
        "\n",
        "steps = 0\n",
        "\n",
        "for step in range(ENTRY_IDX, len(returns)):\n",
        "  today_trend = trend_direction[step]\n",
        "  yday_trend  = trend_direction[step - 1]\n",
        "  today_price = prices[step]\n",
        "  macd_yesterday = macd[step - 1] - ema9[step]\n",
        "  macd_today = macd[step] - ema9[step]\n",
        "  signal = 'hold'\n",
        "  if macd_yesterday > 0 and macd_today < 0:\n",
        "    signal = 'buy'\n",
        "  elif macd_yesterday < 0 and macd_today > 0:\n",
        "    signal = 'sell'\n",
        "\n",
        "  if today_trend == 'down' and signal == 'buy':\n",
        "    signal = 'hold'\n",
        "\n",
        "  if today_trend == 'down' and yday_trend != 'down':\n",
        "    signal = 'sell'\n",
        "\n",
        "  if today_trend == 'down' and signals[-1] == 'buy':\n",
        "    signal = 'sell'\n",
        "\n",
        "  if signal == 'buy':\n",
        "    if today_price >= last_sales_price:\n",
        "      signal = 'hold'\n",
        "\n",
        "  if signal == 'sell':\n",
        "    if today_price <= last_buy_price:\n",
        "      signal = 'hold'\n",
        "\n",
        "  if signal == 'buy':\n",
        "    affordable_qty = floor((cash - TRANS_COST) / today_price)\n",
        "    if affordable_qty >= MIN_PURCHAS:\n",
        "      cash -= today_price * affordable_qty\n",
        "      stocks += affordable_qty\n",
        "      last_buy_price = today_price\n",
        "  if signal == 'sell':\n",
        "    if stocks > 0:\n",
        "      cash += today_price * stocks\n",
        "      stocks = 0\n",
        "      last_sales_price = today_price\n",
        "\n",
        "  equity.append(cash)\n",
        "  inventory.append(stocks)\n",
        "  signals.append(signal)\n",
        "  steps += 1\n",
        "\n",
        "profit = 100 * (equity[-1] - INITIAL_INV) / INITIAL_INV\n",
        "print('Profit: MACD crossover with trend = {:.2f}%'.format(profit))\n",
        "\n",
        "d_now = iuit_l.index[step]\n",
        "print('Date = {}, price = {:.4f}, signal = {}'.format(d_now, today_price, signal))"
      ],
      "metadata": {
        "id": "i5Xz1VV5VW3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plotPrices(prices, signals, ENTRY_IDX, None, None)"
      ],
      "metadata": {
        "id": "TT2TzSVQe_wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ARMA-GARCH"
      ],
      "metadata": {
        "id": "SWpYBWi-A3YI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def computeAcfValue(diffs, n_lags):\n",
        "  acf_res = pd.DataFrame(acf(pd.Series(diffs), nlags=n_lags))\n",
        "  acf_res.columns = ['value']\n",
        "  # ACF confidence interval is not equal to the marked region of the ACF plot\n",
        "  # since the region is the critical region and it is computed differently from\n",
        "  # the confidence interval. Source:\n",
        "  # https://stats.stackexchange.com/questions/185425/how-to-determine-the-critical-values-of-acf\n",
        "  # formula: 1.96 / sqrt(T - d), where T -- number of observations, d - lag\n",
        "  critical_region = []\n",
        "  for lag in range(1, n_lags + 2):\n",
        "    critical_region.append(np.round(1.96 / np.sqrt(len(diffs) - lag), 5))\n",
        "  acf_res['region'] = critical_region\n",
        "  acf_res['abs_val'] = abs(acf_res['value'])\n",
        "  acf_res = acf_res[acf_res.index != 0]\n",
        "  signif_idx = acf_res.index[acf_res['abs_val'] >= acf_res['region']].tolist()\n",
        "  if len(signif_idx) > 0:\n",
        "    idx_list = [1 if x in signif_idx else 0 for x in range(1, max(signif_idx) + 1)]\n",
        "  else:\n",
        "    idx_list = [0]\n",
        "  idx_list   = tuple(idx_list)\n",
        "  return idx_list\n",
        "\n",
        "def computePacfValue(diffs, n_lags):\n",
        "  pacf_res = pd.DataFrame(pacf(pd.Series(diffs), nlags=n_lags))\n",
        "  pacf_res.columns = ['value']\n",
        "  critical_region  = []\n",
        "  for lag in range(1, n_lags + 2):\n",
        "    critical_region.append(np.round(1.96 / np.sqrt(len(diffs) - lag), 5))\n",
        "  pacf_res['region']  = critical_region\n",
        "  pacf_res['abs_val'] = abs(pacf_res['value'])\n",
        "  pacf_res = pacf_res[pacf_res.index != 0]\n",
        "  signif_idx = pacf_res.index[pacf_res['abs_val'] >= pacf_res['region']].tolist()\n",
        "  if len(signif_idx) > 0:\n",
        "    idx_list = [1 if x in signif_idx else 0 for x in range(1, max(signif_idx) + 1)]\n",
        "  else:\n",
        "    idx_list = [0]\n",
        "  idx_list   = tuple(idx_list)\n",
        "  return idx_list\n",
        "\n",
        "def findAcfValue(acf_val):\n",
        "  \"\"\"\n",
        "  This function takes a sequence of ACF lags starting from 1 and up to n-lags to\n",
        "  return the number of lags as a list of indices that are equal to 1. In case\n",
        "  the length of the ACF is 1, then it just returns the first value.\n",
        "  \"\"\"\n",
        "  new_acf = 0\n",
        "  if len(acf_val) == 1:\n",
        "    new_acf = acf_val[0]\n",
        "  else:\n",
        "    new_acf = [i + 1 for i,x in enumerate(acf_val) if x == 1]\n",
        "  if new_acf == 0:\n",
        "    new_acf = [new_acf]\n",
        "  return new_acf\n",
        "\n",
        "\n",
        "def prepareOrders(max_p, max_q, ar, ma):\n",
        "  \"\"\"\n",
        "  This function generates a list of candidate orders for ARMA model by combining\n",
        "  pre-computed ACF and PACF values with default orders and removing (0, 0) to\n",
        "  prevent random walk.\n",
        "  ar -- pre-computed ACF value\n",
        "  ma -- pre-computed PACF value\n",
        "  Returns a list of tuples where the first value in a tuple is for P, the second\n",
        "  is for Q.\n",
        "  \"\"\"\n",
        "  orders = list(product(np.append(np.arange(max_p + 1), ar),\n",
        "                        np.append(np.arange(max_q + 1), ma)))\n",
        "  orders.remove((0, 0))\n",
        "  return orders\n",
        "\n",
        "def simultaneousLlh(r, orders):\n",
        "  \"\"\"\n",
        "  This function employs the SLSQP optimization algorithm to simultaneously\n",
        "  determine the optimal orders of ARMA and GARCH models, evaluating log\n",
        "  likelihood using epsilon from ARMA and sigma from GARCH, and returning the\n",
        "  P and Q values minimizing AIC.\n",
        "  \"\"\"\n",
        "  best_aic = np.inf\n",
        "  for order in orders:\n",
        "    p = order[0]\n",
        "    q = order[1]\n",
        "    result = trainModel(r, p, q)\n",
        "    current_aic = 2 * result[1] + 2 * len(result[0])\n",
        "    if current_aic < best_aic:\n",
        "      best_aic = current_aic\n",
        "      best_p, best_q = p, q\n",
        "      best_params = result[0]\n",
        "  return best_p, best_q\n",
        "\n",
        "def trainArma(r, p, q, max_p, max_q):\n",
        "  \"\"\"\n",
        "  This function trains an ARMA model with specified P and Q parameters. If\n",
        "  either parameter exceeds the predetermined threshold, the model transforms\n",
        "  them into a list to represent standalone t - n lags, as described in\n",
        "  the SARIMAX model.\n",
        "  \"\"\"\n",
        "  if p > max_p:\n",
        "    p = [p]\n",
        "  if q > max_q:\n",
        "    q = [q]\n",
        "  arima_model = sm.tsa.statespace.SARIMAX(r,\n",
        "                                          trend = 'c',\n",
        "                                          order = (p, 0, q),\n",
        "                                          enforce_stationarity=False,\n",
        "                                          enforce_invertibility=False)\n",
        "  arima_result = arima_model.fit(disp=False)\n",
        "  arima_resids = arima_result.resid\n",
        "  return arima_result, arima_resids\n",
        "\n",
        "def computeEpsilon(c, phi, theta, r):\n",
        "  \"\"\"\n",
        "  This function computes epsilon for using the formula for an ARMA(p, q) process\n",
        "  with intercept and returns an array of epsilons.\n",
        "  c - coefficient of intercept\n",
        "  phi - list of phi coefficients of length p\n",
        "  theta - list of theta coefficients of length q\n",
        "  r - an array of returns\n",
        "  \"\"\"\n",
        "  T = len(r)\n",
        "  eps = np.zeros(T)\n",
        "  for t in range(T):\n",
        "    if t < len(phi):\n",
        "      eps[t] = r[t] - np.mean(r)\n",
        "    else:\n",
        "      ar_part = np.sum(np.array([phi[i] * r[t - 1 - i] for i in range(len(phi))], dtype=np.float64))\n",
        "      ma_part = np.sum(np.array([theta[i] * eps[t - 1- i] for i in range(len(theta))], dtype=np.float64))\n",
        "      eps[t]  = r[t] - c - ar_part - ma_part\n",
        "  return eps\n",
        "\n",
        "def getSigma2(omega, alpha, beta, gamma, r, eps):\n",
        "  T = len(eps)\n",
        "  sigma2 = np.zeros(T)\n",
        "  if (1 - alpha - beta) == 0:\n",
        "    sigma2[0] = omega / (1 - 0.9999999999999998)\n",
        "  else:\n",
        "    sigma2[0] = omega / (1 - alpha - beta)\n",
        "  for t in range(1, T):\n",
        "    sigma2[t] = omega + alpha * eps[t - 1] ** 2 + beta * sigma2[t - 1]\n",
        "  return sigma2\n",
        "\n",
        "def llhNormal(params, p, q, r):\n",
        "  # We need constant, phi and theta for epsilon estimation in ARMA process\n",
        "  c     = params[0]\n",
        "  phi   = params[1:p+1]\n",
        "  theta = params[p+1:p+q+1]\n",
        "  # We need omega, alpha and beta params for GARCH process\n",
        "  omega, alpha, beta = params[-3:]\n",
        "  gamma = None\n",
        "  et = computeEpsilon(c, phi, theta, r)\n",
        "  sigma2 = getSigma2(omega, alpha, beta, gamma, r, et)\n",
        "  if any(sigma2 < 0):\n",
        "    sigma2[sigma2 < 0] = -1 * sigma2[sigma2 < 0]\n",
        "  llh    = -0.5 * (np.log(2*np.pi) + np.log(sigma2) + et**2 / (2*sigma2))\n",
        "  neg_llh   = -llh\n",
        "  total_llh = np.sum(neg_llh)\n",
        "  return total_llh\n",
        "\n",
        "def cons0(params, p, q, r):\n",
        "  alpha, beta = params[-2:]\n",
        "  return 1.0 - np.finfo(np.float64).eps - alpha - beta\n",
        "\n",
        "def cons1(params, p, q, r):\n",
        "  return 1.0 - np.sum(params[1:p+1]) - np.finfo(np.float64).eps\n",
        "\n",
        "def trainModel(r, p, q):\n",
        "  np.seterr(divide='ignore', invalid='ignore', over='ignore')\n",
        "  e = np.finfo(np.float64).eps\n",
        "  bounds = [(-10*np.abs(np.mean(r)), 10*np.abs(np.mean(r)))] + \\\n",
        "           [(-0.999999, 0.999999) for _ in range(p + q)] + \\\n",
        "           [(e, 2 * np.var(r))]\n",
        "  alpha_bounds, beta_bounds = [(e, 1.0 - e) for _ in range(2)]\n",
        "  initial_params = [0.001 for _ in range(p + q + 1)]\n",
        "  initial_params = initial_params + [0.001, 0.1, 0.8]\n",
        "  bounds = bounds + [alpha_bounds, beta_bounds]\n",
        "  min_func = llhNormal\n",
        "  eqcons   = []\n",
        "  ieqcons  = [cons0, cons1]\n",
        "  result = optimize.fmin_slsqp(func = min_func,\n",
        "                               x0   = initial_params,\n",
        "                               ieqcons = ieqcons,\n",
        "                               eqcons  = eqcons,\n",
        "                               bounds  = bounds,\n",
        "                               epsilon = 1e-6,\n",
        "                               acc     = 1e-7,\n",
        "                               full_output = True,\n",
        "                               iprint  = 0,\n",
        "                               args    = (p, q, r),\n",
        "                               iter    = 300)\n",
        "  return result\n",
        "\n",
        "def trainGarch(scaled_training):\n",
        "  archm = arch_model(scaled_training,\n",
        "                     mean = 'zero',\n",
        "                     vol  = 'garch',\n",
        "                     p    = 1,\n",
        "                     q    = 1,\n",
        "                     dist = 'Normal')\n",
        "  gjr = archm.fit(update_freq=100, disp=False)\n",
        "  return gjr\n",
        "\n",
        "def findLengthOfSequences(nums):\n",
        "  \"\"\"\n",
        "  This function finds positions of 'negative' strings in a list and returns them\n",
        "  together with the sequence of 'positive' strings preceeding the negative ones.\n",
        "  The purpose of this function is to produce an idea of how biased the random\n",
        "  process is.\n",
        "  \"\"\"\n",
        "  indexes = []\n",
        "  result  = []\n",
        "  count  = 0\n",
        "  for idx in range(len(nums)):\n",
        "    num = nums[idx]\n",
        "    if num == 'positive':\n",
        "      count += 1\n",
        "    else:\n",
        "      if count > 0:\n",
        "        indexes.append(idx)\n",
        "        result.append(count)\n",
        "      count = 0\n",
        "  if count > 0:\n",
        "    indexes.append(idx)\n",
        "    result.append(count)\n",
        "  return {'indexes': indexes, 'counts':result}"
      ],
      "metadata": {
        "id": "5sFcvj6e_nJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test how long can the period of positive return can be before it is replaced\n",
        "# with negative to determine the sales point\n",
        "seqs = ['positive' if x >= 0 else 'negative' for x in returns[:ENTRY_IDX]]\n",
        "sign_seqs = pd.DataFrame(findLengthOfSequences(seqs))\n",
        "\n",
        "pos = seqs.count('positive')\n",
        "neg = seqs.count('negative')\n",
        "print(pos, neg)"
      ],
      "metadata": {
        "id": "rrfhd1nTvMJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = sign_seqs.groupby('counts').size().rename('frequency').reset_index()"
      ],
      "metadata": {
        "id": "ndrTG3gZ3nqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = 0\n",
        "\n",
        "\n",
        "# TODO: change to upper and lower history\n",
        "\n",
        "test_h = []\n",
        "up_h   = []\n",
        "down_h = []\n",
        "\n",
        "for step in range(ENTRY_IDX, last_idx):\n",
        "  window_rets = returns[step - LOOK_BACK:step + LOOK_AHEAD]\n",
        "  train = window_rets[:LOOK_BACK]\n",
        "  test  = window_rets[-LOOK_AHEAD:]\n",
        "\n",
        "  p_max = max(findAcfValue(computeAcfValue(train, 20)))\n",
        "  q_max = max(findAcfValue(computePacfValue(train, 20)))\n",
        "\n",
        "  candidates = prepareOrders(MAX_P, MAX_Q, p_max, q_max)\n",
        "\n",
        "  p, q = simultaneousLlh(train, candidates)\n",
        "  arima_result, arima_resids = trainArma(train, p, q, MAX_P, MAX_Q)\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  scaled_train = scaler.fit_transform(np.array(arima_resids).reshape(-1, 1))\n",
        "  gjr = trainGarch(scaled_train)\n",
        "\n",
        "  predicted_mu = arima_result.forecast(steps=LOOK_AHEAD).tolist()\n",
        "\n",
        "  predicted_et = gjr.forecast(horizon=len(test)).variance\n",
        "  predicted_et = scaler.inverse_transform(predicted_et).tolist()[0]\n",
        "\n",
        "\n",
        "  y_hat = [mu + et for mu, et in zip(predicted_mu, predicted_et)]\n",
        "\n",
        "  steps += 1\n",
        "  if steps == 1:\n",
        "    break"
      ],
      "metadata": {
        "id": "2gr2gZKqCIhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(tmp['test'], color='grey')\n",
        "plt.plot(tmp['pred'], color='green')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dxc0Dm0cU2vx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}